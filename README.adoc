== Stream Applications

In this repository, you will find a collection of components that can meet various data integration use cases and requirements.

The repository's primary focus is to provide a set of standalone Java functions that can be useful in the end-user
applications as-is.

Besides, this repository builds on the Java functions to generate standalone Spring Cloud Stream applications that can run
against Spring Cloud Stream's RabbitMQ or Apache Kafka binder implementations. It is also possible to extend the generator
to bundle the Java functions with the other supported binder implementations.

These applications can run standalone or as part of a data flow, such as the one orchestrated using Spring Cloud Data Flow.

=== Project Structure

The repository includes two sections - `Functions` and `Applications`. The former hosts the various Java functions, and
the latter is for generating the standalone Spring Cloud Stream applications.

The following are the four major components of this repository.

* https://github.com/spring-cloud/stream-applications/tree/master/functions[Standalone Java Functions]
* https://github.com/spring-cloud/stream-applications/tree/master/applications/stream-applications-core[Common Core]
* https://github.com/spring-cloud/stream-applications/tree/master/applications[Spring Cloud Stream Applications]
* https://github.com/spring-cloud/stream-applications/tree/master/applications/stream-applications-build[Docs / Tools]

=== Reusable Functions

|===
| `java.util.Supplier` | `java.util.Function` | `java.util.Consumer`

|link:functions/supplier/file-supplier/README.adoc[File]
|link:functions/function/filter-function/README.adoc[Filter]
|link:functions/consumer/cassandra-consumer/README.adoc[Cassandra]
|link:functions/supplier/ftp-supplier/README.adoc[FTP]
|link:functions/function/header-enricher-function/README.adoc[Header-Enricher]
|link:functions/consumer/analytics-consumer/README.adoc[Analytics]
|link:functions/supplier/geode-supplier/README.adoc[Geode]
|link:functions/function/http-request-function/README.adoc[HTTP Request]
|link:functions/consumer/file-consumer/README.adoc[File]
|link:functions/supplier/http-supplier/README.adoc[HTTP]
|link:functions/function/image-recognition-function/README.adoc[Image Recognition(Tensorflow)]
|link:functions/consumer/ftp-consumer/README.adoc[FTP]
|link:functions/supplier/jdbc-supplier/README.adoc[JDBC]
|link:functions/function/object-detection-function/README.adoc[Object Detection(Tensorflow)]
|link:functions/consumer/geode-consumer/README.adoc[Geode]
|link:functions/supplier/jms-supplier/README.adoc[JMS]
|link:functions/function/semantic-segmentation-function/README.adoc[Semantic Segmentation(Tensorflow)]
|link:functions/consumer/jdbc-consumer/README.adoc[JDBC]
|link:functions/supplier/mongodb-supplier/README.adoc[MongoDB]
|link:functions/function/spel-function/README.adoc[SpEL]
|link:functions/consumer/log-consumer/README.adoc[Log]
|link:functions/supplier/mqtt-supplier/README.adoc[MQTT]
|link:functions/function/splitter-function/README.adoc[Splitter]
|link:functions/consumer/mongodb-consumer/README.adoc[MongoDB]
|link:functions/supplier/rabbit-supplier/README.adoc[RabbitMQ]
|link:functions/function/task-launch-request-function/README.adoc[Task Launch Request]
|link:functions/consumer/mqtt-consumer/README.adoc[MQTT]
|link:functions/supplier/s3-supplier/README.adoc[AWS S3]
|link:functions/function/tasklauncher-function/README.adoc[Task Launcher]
|link:functions/consumer/rabbit-consumer/README.adoc[RabbitMQ]
|link:functions/supplier/sftp-supplier/README.adoc[SFTP]
|link:functions/function/twitter-function/README.adoc[Twitter]
|link:functions/consumer/redis-consumer/README.adoc[Redis]
|link:functions/supplier/tcp-supplier/README.adoc[TCP]
|
|link:functions/consumer/s3-consumer/README.adoc[AWS S3]
|link:functions/supplier/time-supplier/README.adoc[Time]
|
|link:functions/consumer/sftp-consumer/README.adoc[SFTP]
|link:functions/supplier/twitter-supplier/README.adoc[Twitter]
|
|link:functions/consumer/tcp-consumer/README.adoc[TCP]
|link:functions/supplier/websocket-supplier/README.adoc[Websocket]
|
|link:functions/consumer/twitter-consumer/README.adoc[Twitter]
|
|
|link:functions/consumer/websocket-consumer/README.adoc[Websocket]
|
|
|link:functions/consumer/wavefront-consumer/README.adoc[Wavefront]
|===

=== Reusable Spring Cloud Stream Applications

|===
| Source | Processor | Sink

|link:applications/source/file-source/README.adoc[File]
|link:applications/processor/bridge-processor/README.adoc[Bridge]
|link:applications/sink/cassandra-sink/README.adoc[Cassandra]
|link:applications/source/ftp-source/README.adoc[FTP]
|link:applications/processor/filter-processor/README.adoc[Filter]
|link:applications/sink/analytics-sink/README.adoc[Analytics]
|link:applications/source/geode-source/README.adoc[Geode]
|link:applications/processor/groovy-processor/README.adoc[Groovy]
|link:applications/sink/file-sink/README.adoc[Fiile]
|link:applications/source/http-source/README.adoc[HTTP]
|link:applications/processor/header-enricher-processor/README.adoc[Header-Enricher]
|link:applications/sink/ftp-sink/README.adoc[FTP]
|link:applications/source/jdbc-source/README.adoc[JDBC]
|link:applications/processor/http-request-processor/README.adoc[HTTP Request]
|link:applications/sink/geode-sink/README.adoc[Geode]
|link:applications/source/jms-source/README.adoc[JMS]
|link:applications/processor/image-recognition-processor/README.adoc[Image Recognition(Tensorflow)]
|link:applications/sink/jdbc-sink/README.adoc[JDBC]
|link:applications/source/load-generator-source/README.adoc[Load-Generator]
|link:applications/processor/object-detection-processor/README.adoc[Object Detection(Tensorflow)]
|link:applications/sink/log-sink/README.adoc[Log]
|link:applications/source/mongodb-source/README.adoc[MongoDB]
|link:applications/processor/semantic-segmentation-processor/README.adoc[Semantic Segmentation(Tensorflow)]
|link:applications/sink/mongodb-sink/README.adoc[MongoDB]
|link:applications/source/mqtt-source/README.adoc[MQTT]
|link:applications/processor/script-processor/README.adoc[Script]
|link:applications/sink/mqtt-sink/README.adoc[MQTT]
|link:applications/source/rabbit-source/README.adoc[RabbitMQ]
|link:applications/processor/splitter-processor/README.adoc[Splitter]
|link:applications/sink/rabbit-sink/README.adoc[RabbitMQ]
|link:applications/source/s3-source/README.adoc[AWS S3]
|link:applications/processor/transform-processor/README.adoc[Transform]
|link:applications/sink/redis-sink/README.adoc[Redis]
|link:applications/source/sftp-source/README.adoc[SFTP]
|link:applications/processor/twitter-trend-processor/README.adoc[Twitter Trend]
|link:applications/sink/router-sink/README.adoc[Router]
|link:applications/source/tcp-source/README.adoc[TCP]
|
|link:applications/sink/sftp-sink/README.adoc[SFTP]
|link:applications/source/time-source/README.adoc[Time]
|
|link:applications/sink/tasklauncher-sink/README.adoc[Task Launcher]
|link:applications/source/twitter-message-source/README.adoc[Twitter Message]
|
|link:applications/sink/tcp-sink/README.adoc[TCP]
|link:applications/source/twitter-search-source/README.adoc[Twitter Search]
|
|link:applications/sink/throughput-sink/README.adoc[Throughput]
|link:applications/source/twitter-stream-source/README.adoc[Twitter Stream]
|
|link:applications/sink/twitter-message-sink/README.adoc[Twitter Message]
|link:applications/source/websocket-source/README.adoc[Websocket]
|
|link:applications/sink/twitter-update-sink/README.adoc[Twitter Update]
|
|
|link:applications/sink/wavefront-sink/README.adoc[Wavefront]
|===

=== Composable Functions

Spring Cloud Stream includes integration with Spring Cloud Function's function-based programming model that lets the
business logic of an application be modeled as a `java.util.Supplier`, a `java.util.Function`, and a `java.util.Consumer`,
representing the roles of a `Source`, a `Processor`, and a `Sink`, respectively.

Building on this foundation, we can extend existing `Source` and `Sink` applications by importing the configuration of an
existing `Source` or `Sink` and adding code that defines a `java.util.Function` — this delivers a lot of powerful composition
possibilities.

Take for instance, the `Source` applications are auto-configured with link:functions/function[functions], which may optionally
be included in a composite function definition.

With this, the same Source application can potentially do one or all of the following without having to build it out as a
standalone processor.

- execute SpEL transformations
- enrich message headers
- filter events
- produce task launch requests on upstream events

For example, the `time` source when it is running, as shown below, will perform a series of internal transformations to
finally publish a task launch request every second to the rabbit exchange with the name `time-test`.

```
java -jar target/time-source-rabbit-3.0.0-SNAPSHOT.jar \
     --spring.cloud.stream.bindings.output.destination=time-test \
     --spring.cloud.stream.function.definition="timeSupplier|spelFunction|headerEnricherFunction|taskLaunchRequestFunction" \
     --spel.function.expression="payload.length()" \
     --header.enricher.headers=task-id=payload*2 \
     --task.launch.request.task-name-expression="'task-'+headers['task-id']"
```

Now, the transformed message would look like:

```
headers:
task-id:   34
content_type:  application/json
Payload
49 bytes
Encoding: string
{"args":[],"deploymentProps":{},"name":"task-34"}
```

Let us unpack what is happening behind the scenes. We will start with the following function definition.

`timeSupplier|spelFunction|headerEnricherFunction|taskLaunchRequestFunction`

- Here, the function definition creates a composite `Supplier` beginning with the default `timeSupplier` Java function included
in this repository, which is the foundation for `time-source`.

- The `spelFunction` applies to a `Message` from which we can extract and transform the `payload` or `headers`, by following
standard Spring Integration conventions.

- The output of `spelFunction` is the length of the date-time String, `17`.

- From here, we apply the header-enricher Java function to add a Message header, `task-id` with the value of `payload*2`.
That would be `34`.

- We use the `task-id` in the header to generate the "task name", and to programmatically derive the task launch request
using the SpEL expression "'task-'+headers['task-id']", or `task-34`.

This somewhat contrived example, but the goal here was to highlight the power of function composition.

If you have had a task definition in Spring Cloud Data Flow with the name `task-34`, you could build a `time | tasklauncher`
streaming data pipeline to launch that task every second.

Before the `3.0` release of Stream Applications, this composition required extensive customization. And a lot more manual
configuration changes, extensions, and custom build of the applications.

NOTE: Support for composite functions includes auto-configuration for conventional binding name mappings (`input` and `output`)
derived from the function definition and the presence of `spring.cloud.stream.bindings.output...`.

In this example, `--spring.cloud.stream.bindings.output.destination=time-test` is enabled behind the scenes by the auto-configured
property
`--spring.cloud.stream.function.bindings.timeSupplierspelFunctionheaderEnricherFunctiontaskLaunchRequestFunction-out-0=output`.

=== Build

You can build everything from the root of the repository.

`./mvnw clean install`

However, this may not be what you are interested in since you are probably interested in a single application or a few of them.

To build the functions and applications that you are interested in, you need to build them selectively, as shown below.

==== Building Functions

`./mvnw clean install -f functions`

You can also build a single function or group of functions.
For example, if you are only interested in `jdbc-supplier` and `log-consumer`, do the following.

`./mvnw clean install -pl :jdbc-suppler,:log-consumer`

==== Building Stream Applications Core

If you want to re-run the common core build, you can build it with the following.

`./mvnw clean install -f applications/stream-applications-core`

=== Building Stream applications

Let's assume that you want to build a `jdbc-source` application based on Kafka Binder in Spring Cloud Stream and Log Sink
application based on Rabbit binder.

Here is what you need to do.
Assuming that you already built both functions and stream-applications-core as above,

```
./mvnw clean package -pl :jdbc-source
cd applications/source/jdbc-source/apps/jdbc-source-kafka
./mvnw clean package
```

This will generate the Kafka binder based uber jar in the target folder.

Similarly, for the `log-sink`, do the following.

```
./mvnw clean package -pl :log-sink
cd applications/sink/log-sink/apps/log-sink-rabbit
./mvnw clean package
```

=== Contributing a new function component to the repository

If you are not finding a function that you are interested in, we encourage you to write that function by yourself and contribute to the repository.
Below, you can find detailed instructions on how one may contribute a new function.

==== Decide what kind of function that you want to write

   a. Do you want to produce information within your application? (poll an external source, receive data on demand etc.)
      If so, then you need to write a `java.util.function.Supplier`.
   b. Does your use case require you to write an application that consumes data and then send that to an external system (or other kind of operations on the received data)?
      In that case, you need to write a `java.util.function.Consumer`.
   c. Do you need to consume data, process it and then produce in the application?
      In that case, you need to write a `java.util.function.Function`.

If your functions are very trivial and don't need any Spring components, you can certainly start there.
For example, here is a simple echo function.

```
public class EchoFunction implements Function<String, String> {
	@Override
	public String apply(String s) {
		return s;
	}
}
```

However, it is likely that your enterprise use cases are much more complex than this and you need additional capability.
You probably need to add extra configuration properties, create extra Spring beans and use them inside your functions and make an existing Spring Integration adapter fit into this functional model etc.
In the following sections, we go through these various scenarios and guide you through writing such a function.
Keep in mind that, we are only describing the basic framework required for contributing a function.
The implementation details need to be thought through and well defined before you can contribute a pull request.

==== Writing a new Supplier function

===== Retrieving data from an external system on each invocation of the Supplier.

This is a pattern in which a Supplier when invoked gives you the data from an external system.
Here is a simple form of how that Supplier might look like.

```
  @Bean
  public Supplier<String> mySupplier() {
     return () -> {
       //here you write code for how you get data.
     }
  }
```

Here is an example: link:functions/supplier/time-supplier/src/main/java/org/springframework/cloud/fn/supplier/time/TimeSupplierConfiguration.java[TimeSupplierConfiguration].

If you want to add additional information as part of the payload produced (for e.g adding some headers), then instead of returning `Supplier<String>`, you can change the signature to `Supplier<Message<?>>`.

Then inside your application, you can call `mySupplier().get()` and retrieve the data.

===== Supplying data using reactive patterns.

Below, we are discussing two patterns using which you can write reactive suppliers.

a. Let's say that you have a Spring Integration based message driven adapter that you want to recieva data as soon as it is available.
You want to produce this data through a supplier for your clients.
You can use the following primitives as a starting point for that.
We use the TCP adapter as an example here:

```

@Bean
public Supplier<Flux<Message<?>>> tcpSupplier() {
    return () -> Flux.from(output())
            .doOnSubscribe(subscription -> adapter().start());
}

@Bean
public FluxMessageChannel output() {
    return new FluxMessageChannel();
}

@Bean
public TcpReceivingChannelAdapter adapter() {
    TcpReceivingChannelAdapter adapter = new TcpReceivingChannelAdapter();
    adapter.setOutputChannel(output());
    adapter.setAutoStartup(false);
    ...
    return adapter;
}
```

b. If you have a custom mechanism (non Spring Integration) and want to produce data reactively, you can follow the blueprint below.

```
@Bean
public Publisher<Message<byte[]>> myPublisher() {
    ...
}

@Bean
public Supplier<Flux<Message<?>>> mySupplier() {
    return () -> Flux.from(myPublisher())
            .doOnSubscribe(subscription -> adapter().start());
}
```

Please take a look at the various suppliers provided link:functions/supplier[here] more usage examples of these various types of suppliers.

===== Common conventions used when writing a Supplier

Following are the expected conventions when writing a new Supplier, taking Google Spanner as an example.

* The name of the configuration class - `SpannerSupplierConfiguration`
* The name of the supplier method - `spannerSupplier`.
* `ConfigurationProperties` class is named as - `SpannerSupplierProperties`.

==== Writing a new Consumer function

Writing a consumer functions is more straightforward than writeing a `Supplier`.
You can simple provide a `Consumer` bean as below.

```
@Bean
public Consumer<String> myConsumer() {
    return data -> sendDataT();
}
```

Or if you are relying on some Spring Integration message handlers,

```
@Bean
public Consumer<Message<?>> fileConsumer() {
    return fileWritingMessageHandler()::handleMessage;
}
```

link:functions/consumer[Here] are all the consumer functions we currently have.

===== Common conventions used when writing a Consumer

Following are the expected conventions when writing a new Consumer, taking Google Spanner as an example.

* The name of the configuration class - `SpannerConsumeronfiguration`
* The name of the supplier method - `spannerConsumer`.
* `ConfigurationProperties` class is named as - `SpannerConsumerProperties`.

==== Writing a new Function

Imaging a use case in which you are consuming data, but then you want to process it and then produce that information.
You can write a `java.util.function.Function` as shown below.

```
@Bean
public Function<Message<?>, Message<?>> myFunction() {
    return message -> transform(message);
}
```

link:functions/function[Here] are all the functions we currently have.

===== Common conventions used when writing a Function

Following are the expected conventions when writing a new Function, taking Uppercase function as an example.

* The name of the configuration class - `UppercaseFunctiononfiguration`
* The name of the supplier method - `uppercaseFunction`.
* `ConfigurationProperties` class is named as - `UppercaseFunctionProperties`.

=== Testing

There is no single strategy for unit testing these functions.
Based on what your use case is and what type of function variant you write, you can employ varying levels of testing capabilities.
Please take a look at the various tests we provide as part of the functions and pick the one that works for you.

=== Other things to keep mind

If your function module has `ConfigurationProperties`, make sure to add the `spring-boot-configuration-processor` dependency.
It is probably better to start with the maven configuration for an existing function.
If your modules have Spring in them, make sure to use `spring-functions-parent` as the parent, otherwise you can use `java-functions-parent`.

You can prepare a pull request at this point for the function, but if you want to generate Spring Cloud Stream applications from the function you wrote, please read on.

=== Generating Spring Cloud Stream applications

Now that you have a function, you can generate Spring Cloud Stream applications based on that.
All the application generation modules are using `stream-applications-core` as the parent.

For the most part, you can simply generate the applications based on a cookie cutter pattern.
Take a look at the many examples we provide link:applications/source[sources], link:applications/sink[sinks] or link:applications/processor[processors].

If you have custom code to add at the application level, then it becomes a bit more involved, but still relatively straight forward.
link:applications/processor/image-recognition-processor[Here] is an example for how you can add extra customizations at the application level.
Pay attention to the code that is changed there and the maven configuration.

If you don't see the need for anything at the function layer and want to write a straight up Spring Cloud Stream application, then that pattern is supported as well.
Here is an link:applications/processor/bridge-processor[example] that doest exactly that.

==== Testing a Spring Cloud Stream application

It is important that you add some basic tests using the Spring Cloud Stream test binder as part of your application.
The parent will provide the test binder as a transitive dependency.
Source, Sink and Processor applications are tested slightly differently.
Take a look at the existing tests provided for more information.

when you are ready, please send a pull request and we will help reviewing it and get that merged.

=== Code of Conduct

Please see our https://github.com/spring-projects/.github/blob/master/CODE_OF_CONDUCT.md[Code of Conduct]
